# -*- coding: utf-8 -*-
"""mini project 1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PHMkP9INxEa195Rd1TAOL5L6R46pMoRU

## **Mini project 1**
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd #importing the required libraries imorting pandas for gettig the dataset and working with dataset.
import numpy as np #importing numpy for working with the array type of data or columns that are coverted into array.
import matplotlib.pyplot as plt #imorting matplotlib for visualizing the result  
# %matplotlib inline 
import seaborn as sns #importing seaborn for visualizing the result 
sns.set() #load default seaborn theme

from sklearn.preprocessing import StandardScaler #importing the Standardard Scaler class from reprocessing libraries  
from sklearn.model_selection import train_test_split #importing the libraries for spliting the dataset into train and test data
from sklearn.metrics import classification_report, confusion_matrix #importing the metrics class for creating the cofuson matrics asnnd gerating the classification report

df = pd.read_csv('heart.csv') # reading the dataset 
df.head() # accessing the top 5 rows of the dataset

df.info() # getting the information about the dataset

df.isnull() # checking the null values present in the dataset

df.isnull().sum() #checking the no. of null values present in the coloumns

df_copy = df.copy(deep= True)#copying the dataset into df_copy variable 
df_copy[['trtbps','chol','thalachh','oldpeak']] = df_copy[['trtbps', 'chol','thalachh','oldpeak']].replace(0,np.nan)# making changes into df_copy by 
#replacing 0 with np.nan
print(df_copy.isnull().sum())#finding wether the the df_copy dataset contains the null values or not

df_copy['oldpeak'].fillna(df_copy['oldpeak'].mean(), inplace= True)#filling the mean value in oldpeak coloumn where the value is nan

color_wheel = {1: "red", 2 :'blue'}
colors = df["output"].map(lambda x: color_wheel.get(x+1))
print(df.output.value_counts())
p = df.output.value_counts().plot(kind="bar")

#using heat map to show correlation between each column
sns.heatmap(df.corr())

"""# ML model bulding

"""

x = df.drop('output',axis = 1) #droping the output coloumn 
y = df['output'] #storing the output coloumn in y

y # display contains in y

x # display contains in x

#spliting of dataset using train_test_split funtion 

from sklearn.model_selection import train_test_split, cross_val_score
x_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2, random_state=0)

x_train #display the contains of x_trian variable

x_test #display the contains of x_test variable

y_train #display the contains of y_trian variable

y_test #display the contains of y_test variable

"""# Decision tree"""

from sklearn.tree import DecisionTreeClassifier  # importing the DecisionTreeClassifier to apply decision tree algorithm on train dataset 
dtree =  DecisionTreeClassifier() # Assiging the dtree variable to DecisionTreeClassifier 
dtree.fit(x_train, y_train) # fitting the training data

y_pred = dtree.predict(x_test) # predicting the x_test

print("Classification report \n", classification_report(y_test, y_pred)) # genrating the classification report of y_test and y_pred elements

cm = confusion_matrix(y_test, y_pred) # creating confusion matrix for decision tree algorithm to find accuracy of algorithm

plt.figure(figsize=(5,5)) #setting the figure size 
sns.heatmap(data=cm, linewidths = .5, annot=True, square=True, cmap = 'Blues')  #creating the heatmap for confusion matrix of decision tree 
plt.ylabel("Actual label") #labeling the y axis 
plt.xlabel('Predicted label') #labeling the x axis 
all_sample_title = 'Accuracy Score: {0}'.format(dtree.score(x_test,y_test)) # printing the accurancy of decision tree algorithm 
plt.title(all_sample_title, size=15) #Giving the title to the figure

from sklearn.metrics import roc_curve, roc_auc_score #importing roc_curve, roc_auc_score libraries to obtain the ruc curve and accuracy score 
y_pred_proba = dtree.predict_proba(x_test)[:][:,1] #calculating the probabilities or y_pred values 

df_actual_predicted = pd.concat([pd.DataFrame(np.array(y_test), columns=['y_actual']), pd.DataFrame(y_pred_proba, columns=['y_pred_proba'])], axis=1) #concatinating actual values and probabilities
fpr, tpr, tr = roc_curve(df_actual_predicted['y_actual'], df_actual_predicted['y_pred_proba']) #generating the ROC graphs along with false positive rate, true positive rate and threshold 
auc = roc_auc_score(df_actual_predicted['y_actual'], df_actual_predicted['y_pred_proba'])#calculating the accuracy score using rocc_auc score

plt.plot(fpr,tpr,label = 'AUC = %0.4f' %auc)#ploting auc curve 
plt.plot(fpr,fpr, linestyle='--', color = 'k' )#ploting false positive rate    
plt.xlabel('False positive rate')#labeling the x axis
plt.ylabel('True positive rate')#labeling the y axis
plt.title('ROC curve', size=15)#assigignt he title to the graph
plt.legend()

"""As the auc score > 0.5 so we can conclude that the decision tree classifier had better classified the classes

# Support vector machine
"""

from sklearn import svm  # importing the SVM algorithm function from sklearn library 
svm = svm.SVC(kernel= 'linear', gamma= 'auto', probability=True) 
svm.fit(x_train, y_train)  #fitting the svm model to train dataset

y_pred = svm.predict(x_test)  # storing the prediction in y_pred
print("Classification report :\n", classification_report(y_test,y_pred)) # printing the classification report of SVM predicted values

cm = confusion_matrix(y_test,y_pred) #ceating confusion matrix for SVM algorithm 
plt.figure(figsize=(5,5)) 
sns.heatmap(data= cm, linewidth=.5, annot=True, square = True, cmap = 'Blues')  #diplaying the confusion matrix for SVM algorithm 
plt.ylabel('Actual label') #Assigning the y axis label 
plt.xlabel('Predicted label') # Assiging the x axis label 
all_sample_title = 'Accuracy Score: {0}'.format(svm.score(x_test, y_test)) # finding the accurancy for SVM algorithm 
plt.title(all_sample_title, size = 15) # Giving the title to the figure

from sklearn.metrics import roc_curve, roc_auc_score#importing roc_curve, roc_auc_score libraries to obtain the ruc curve and accuracy score 
y_pred_proba = svm.predict_proba(x_test)[:][:,1]#calculating the probabilities or y_pred values 

df_actual_predicted = pd.concat([pd.DataFrame(np.array(y_test), columns=['y_actual']), pd.DataFrame(y_pred_proba, columns=['y_pred_proba'])], axis=1)#concatinating actual values and probabilities
fpr, tpr, tr = roc_curve(df_actual_predicted['y_actual'], df_actual_predicted['y_pred_proba'])#generating the ROC graphs along with false positive rate, true positive rate and threshold
auc = roc_auc_score(df_actual_predicted['y_actual'], df_actual_predicted['y_pred_proba'])#calculating the accuracy score using rocc_auc score

plt.plot(fpr,tpr,label = 'AUC = %0.4f' %auc)#ploting auc curve 
plt.plot(fpr,fpr, linestyle='--', color = 'k' )#ploting false positive rate    
plt.xlabel('False positive rate')#labeling the x axis
plt.ylabel('True positive rate')#labeling the y axis
plt.title('ROC curve', size=15)#assiging the title to the graph
plt.legend()

"""As the auc score > 0.5 so we can conclude that the SVM classifier had better classified the classes and also if we copare it with decision tree SVM gives better classification.

# **Logistic Regression **
"""

from sklearn.linear_model import LogisticRegression # importing ogistic regression from sklearn library 
logreg = LogisticRegression() # Assigning the logreg classifier to LogisticRegression
logreg.fit(x_train, y_train) # fitting the trianing data

y_pred = logreg.predict(x_test) # Storing the predicted data from test to y_pred 
print("Classiication report :\n", classification_report(y_test,y_pred))  # creating the classification report for Logistics Regression

cm = confusion_matrix(y_test, y_pred) # creating the confusion matrix  for Logistic regresssion
plt.figure(figsize=(5,5))#fixing the figure size 
sns.heatmap(data = cm, linewidth = .5, annot = True, square= True, cmap = 'Blues') # displaying the confusion matrix using heat map 
all_sample_title = 'Accuracy Score : {0}'.format(logreg.score(x_test,y_test)) # printing the accuracy for the Logistic model using test data 
plt.title(all_sample_title, size=15) #assignign the title to the figure

from sklearn.metrics import roc_curve, roc_auc_score #importing roc_curve, roc_auc_score libraries to obtain the ruc curve and accuracy score
y_pred_proba = logreg.predict_proba(x_test)[:][:,1]#calculating the probabilities or y_pred values 

df_actual_predicted = pd.concat([pd.DataFrame(np.array(y_test), columns=['y_actual']), pd.DataFrame(y_pred_proba, columns=['y_pred_proba'])], axis=1)#concatinating actual values and probabilities
df_actual_predicted.index = y_test.index #storinng the index for predicted values 

fpr, tpr, tr = roc_curve(df_actual_predicted['y_actual'], df_actual_predicted['y_pred_proba'])#generating the ROC graphs along with false positive rate, true positive rate and threshold
auc = roc_auc_score(df_actual_predicted['y_actual'], df_actual_predicted['y_pred_proba'])#calculating the accuracy score using rocc_auc score

plt.plot(fpr,tpr,label = 'AUC = %0.4f' %auc)#ploting auc curve 
plt.plot(fpr,fpr, linestyle='--', color = 'k' )#ploting false positive rate  
plt.xlabel('False positive rate')#labeling the x axis 
plt.ylabel('True positive rate')#labeling the y axis 
plt.title('ROC curve', size=15)#giving title to graph
plt.legend()

"""As the auc score > 0.5 so we can conclude that the logistic regression classifier had better classified the classes and also if we compare it with decision tree and SVM it gives better classification.

# KNN Algorithm
"""

from sklearn.neighbors import KNeighborsClassifier  # importing the KNeighborsClassifier from sklearn library 

knn = KNeighborsClassifier() # Creating the classifier for KNeighborsClassifier 
knn.fit(x_train,y_train) # fitting the trianing data

y_pred = knn.predict(x_test) # Storing the predicted data from x_test in y_pred 
print("Classification Report: \n", classification_report(y_test, y_pred)) # creating the classification report for KNN algorithm

cm = confusion_matrix(y_test,y_pred) # Creating the Confusion matrix for KNN
plt.figure(figsize = (5,5))#setting the size of figure 
sns.heatmap(data = cm, linewidth = .5, annot = True, cmap = 'Blues') #displaying the confusion matrix using heat map function 
plt.ylabel('Actual label') #Assigning the y axis label
plt.xlabel('Predicted label') #Assigning the x axis label
all_sample_title = 'Accuracy Score : {0}'.format(knn.score(x_test, y_test)) # calculating the accuracy Score for KNN
plt.title(all_sample_title, size = 15) # assiging the title to the heatmap

from sklearn.metrics import roc_curve, roc_auc_score #importing roc_curve, roc_auc_score libraries to obtain the ruc curve and accuracy score
y_pred_proba = knn.predict_proba(x_test)[:][:,1]#calculating the probabilities or y_pred values 

df_actual_predicted = pd.concat([pd.DataFrame(np.array(y_test), columns=['y_actual']), pd.DataFrame(y_pred_proba, columns=['y_pred_proba'])], axis=1)#concatinating actual values and probabilities
df_actual_predicted.index = y_test.index

fpr, tpr, tr = roc_curve(df_actual_predicted['y_actual'], df_actual_predicted['y_pred_proba'])#generating the ROC graphs along with false positive rate, true positive rate and threshold
auc = roc_auc_score(df_actual_predicted['y_actual'], df_actual_predicted['y_pred_proba'])#calculating the accuracy score using rocc_auc score

plt.plot(fpr,tpr,label = 'AUC = %0.4f' %auc)#ploting auc curve 
plt.plot(fpr,fpr, linestyle='--', color = 'k' )#ploting false positive rate  
plt.xlabel('False positive rate')#labeling the x axis 
plt.ylabel('True positive rate')#labeling the y axis 
plt.title('ROC curve', size=15)#giving title to graph
plt.legend()

"""As the auc score > 0.5 so we can conclude that the knn classifier gives good classified the classes and also if we compare it with other three classifiers it gives low results

# **Conclusion **
In this mini project we have applied different classifiers like Decison Tree, SVM, KNN algorithm and logistic regression on heart attack dataset. By applying this classifiers we have obersverd that Logistic regression have the highest accuracy i.e. 85% and KNN algorthm having lowest acccuracy with 66%. So we can conclude that among these four algorithms logistic algorithm has given more precise output for the heart attack dataset.
"""